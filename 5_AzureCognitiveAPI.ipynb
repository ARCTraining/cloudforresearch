{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Azure cognitive services APIs\n",
    "\n",
    "These are just a few code snippetts to get you started with the Azure cognitive services. Use the ones you like and see how you go!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1. Azure Storage API\n",
    "Almost all Cognitive Services API require input data. i.e. image data to detect faces. This section will show how to use the Azure Storage API to store data in your own cloud storage service with Python APIs.\n",
    "\n",
    "#### Sign-up to the Azure Storage Service\n",
    "1. Create an Azure Storage account:\n",
    "    1. Go to the Azure Portal at http://portal.azure.com/ and sign in with your Azure account credentials\n",
    "    2. Click the [New] icon on the top left of the Portal, then click Data + Storage > Storage Account. \n",
    "    Click the [Create] button, then give the storage account a unique name and create a new resource group for it. \n",
    "    \n",
    "    2. When the storage account has been created, the Notifications button will flash a green SUCCESS flag and the storage account's 'blade' is open to show that it belongs to the new resource group you created.\n",
    "    3. Click the **Access keys** part in the storage account's blade. Take note of the **account name** and **key1**.\n",
    "\n",
    "2. Using open source, cross-platform [\"Storage Explorer\"](http://storageexplorer.com/) tool and the above **account name** and **key1**, create a container under your storage service.\n",
    "\n",
    "3. You can find the sample files IN THE APPENDIX of this repository. Upload these or similar own files into your Azure storage account, so they cn be used in the python sample codes below.\n",
    "\n",
    "These activities use the [Microsoft Azure Storage SDK for Python](https://github.com/Azure/azure-storage-python) to access your Azure Storage Service.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your credentials from above\n",
    "\n",
    "azure_storage_account_name = 'enterhere'\n",
    "azure_storage_account_key = 'enterhere'  # dont need key... we will access public blob... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT\n",
    "This line installs the AZURE STORAGE ACCOUNT LIBRARIES required to access Azure Blob Storage Service. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-storage==0.32.0\n",
      "  Downloading azure_storage-0.32.0-py3-none-any.whl (160kB)\n",
      "\u001b[K    100% |################################| 163kB 3.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: azure-common in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from azure-storage==0.32.0)\n",
      "Requirement already satisfied: requests in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from azure-storage==0.32.0)\n",
      "Requirement already satisfied: azure-nspkg in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from azure-storage==0.32.0)\n",
      "Requirement already satisfied: python-dateutil in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from azure-storage==0.32.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from python-dateutil->azure-storage==0.32.0)\n",
      "Installing collected packages: azure-storage\n",
      "  Found existing installation: azure-storage 0.34.3\n",
      "    Uninstalling azure-storage-0.34.3:\n",
      "      Successfully uninstalled azure-storage-0.34.3\n",
      "Successfully installed azure-storage-0.32.0\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-storage==0.32.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlockBlobService\n",
    "\n",
    "# create blob service object to access the files in the storage\n",
    "# You can access your account_name and account_key values at [Azure Management Portal](https://portal.azure.com)\n",
    "blob_service = BlockBlobService(azure_storage_account_name, azure_storage_account_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "List sample data files in a specific container (folder), so we can select one of them to work on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# select container (folder) name where the files resides\n",
    "container_name = 'sampledata'\n",
    "\n",
    "# list files in the selected folder\n",
    "generator = blob_service.list_blobs(container_name)\n",
    "\n",
    "blob_prefix = 'https://{0}.blob.core.windows.net/{1}/{2}'\n",
    "\n",
    "print(\"List of files in the container:\")\n",
    "for blob in generator:\n",
    "    print(blob_prefix.format(blob_service.account_name, container_name, blob.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Read one of the image file from the blob storage as byte array. If you want update the below blob_name variable with one of the file available in the above file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "blob_name = 'emotion1.jpeg'\n",
    "blob = blob_service.get_blob_to_bytes(container_name, blob_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using PIL and matplotlib libraries, display the image file content to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "image_file_in_mem = io.BytesIO(blob)\n",
    "img_bytes = Image.open(image_file_in_mem)\n",
    "plt.imshow(img_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2. SEARCH - Bing Image Search API\n",
    "You can use Bing Image Search API to keyword based image search, get insights about an image, find trending images.  \n",
    "Ref: https://msdn.microsoft.com/en-us/library/dn760791.aspx  \n",
    "     https://www.microsoft.com/cognitive-services/en-us/bing-image-search-api\n",
    "\n",
    "Call Bing Image Search API to get images with human faces (in later sections, will be used for face detection etc.) Change search parameters as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests     # used to make rest calls\n",
    "import json # parse response result\n",
    "import urllib\n",
    "\n",
    "url_search_api = 'https://api.cognitive.microsoft.com/bing/v5.0/images/search?' # service address \n",
    "api_key = 'c8042821455b4dc095cceaa0cd0dc67e'          # Azure Cognitive API Key, replace with your own key\n",
    "\n",
    "headers = {'Ocp-Apim-Subscription-Key':api_key}\n",
    "\n",
    "params = urllib.parse.urlencode({\n",
    "    'q': 'facial emotion',\n",
    "    'count': '5',\n",
    "    'offset': '0',\n",
    "    'mkt': 'en-us',\n",
    "    'safeSearch': 'Moderate',\n",
    "    #'imageContent' : 'Portrait', \n",
    "    'imageType' : 'Photo' # AnimatedGif, Clipart, Line, Photo, Shopping\n",
    "})\n",
    "\n",
    "url = url_search_api + params\n",
    "\n",
    "api_response = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Print out the call response (which is in Json format) in pretty form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "\n",
    "print(json.dumps(res_json, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Display search resulting images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image as ipImage, display\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "for j in res_json['value']:\n",
    "    c_url = j['contentUrl']\n",
    "    pr = urlparse(c_url)\n",
    "    ru = parse_qs(pr.query)\n",
    "    img_url = ru['r'][0]\n",
    "    img = ipImage(url=img_url, width=150, height=200)\n",
    "    \n",
    "    display(img)\n",
    "    print(img_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3. VISION - Face API\n",
    "You can use Face API to Detect, Find Similar, Group, Identify, Verify faces.  \n",
    "Ref: https://www.microsoft.com/cognitive-services/en-us/face-api\n",
    "\n",
    "Call Cognitive Services Face API to detect facial features in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# load image file to process\n",
    "blob_name = 'face4.jpeg'\n",
    "blob = blob_service.get_blob_to_bytes(container_name, blob_name)\n",
    "image_file_in_mem = io.BytesIO(blob)\n",
    "img_bytes = Image.open(image_file_in_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "\n",
    "url_face_api = 'https://api.projectoxford.ai/face/v1.0/detect' # service address \n",
    "api_key = 'd469dbc2ba424afd86c0ba0716a62ca8'          # Azure Cognitive API Key, replace with your own key\n",
    "\n",
    "headers = {'Content-Type': 'application/octet-stream', 'Ocp-Apim-Subscription-Key':api_key}\n",
    "\n",
    "params = urllib.parse.urlencode({\n",
    "    'returnFaceId': 'true',\n",
    "    'returnFaceLandmarks': 'true',\n",
    "    'returnFaceAttributes': 'age,gender,smile,facialHair,headPose,glasses',\n",
    "})\n",
    "\n",
    "query_string = '?{0}'.format(params)\n",
    "\n",
    "url = url_face_api + query_string\n",
    "\n",
    "api_response = requests.post(url, headers=headers, data=blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Print out the call response (which is in Json format) in pretty form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "\n",
    "print(json.dumps(res_json, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Get detected face coordiates (bounding squares) from call response and draw red square borders for females, blue for males."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax = plt.gca()\n",
    "\n",
    "# Draw face rectangles\n",
    "for i in res_json:\n",
    "    fr = i['faceRectangle'] # get faceRectangle node per detected face in the image\n",
    "\n",
    "    pc = 'red' # patch color\n",
    "    if i['faceAttributes']['gender'] == 'male':\n",
    "        pc = 'blue'\n",
    "\n",
    "    ax.add_patch(\n",
    "        patches.Rectangle(\n",
    "            (fr['left'], fr['top']), fr['width'], fr['height'],\n",
    "            fill=False, linewidth=4, color=pc)\n",
    "    )\n",
    "    \n",
    "    ax.text(fr['left'], fr['top']+fr['height'], \n",
    "            'age:'+str(i['faceAttributes']['age']), \n",
    "            fontsize=14, weight='bold', color='red', bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "\n",
    "ps = 5       # patch size\n",
    "pc = '#00FF00'   # patch color\n",
    "\n",
    "# Draw eye, nose, mouth\n",
    "for i in res_json:\n",
    "    fl = i['faceLandmarks']\n",
    "\n",
    "    # left eye\n",
    "    ax.add_patch(patches.Circle((fl['pupilLeft']['x'], fl['pupilLeft']['y']), ps, color=pc))\n",
    "\n",
    "    # right eye\n",
    "    ax.add_patch(patches.Circle((fl['pupilRight']['x'], fl['pupilRight']['y']), ps, color=pc))\n",
    "\n",
    "    # mouth\n",
    "    ax.add_patch(patches.Circle((fl['mouthLeft']['x'], fl['mouthLeft']['y']), ps, color=pc))\n",
    "    ax.add_patch(patches.Circle((fl['mouthRight']['x'], fl['mouthRight']['y']), ps, color=pc))\n",
    "\n",
    "    # nose\n",
    "    ax.add_patch(patches.Circle((fl['noseTip']['x'], fl['noseTip']['y']), ps, color=pc))\n",
    "\n",
    "plt.imshow(img_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4. VISION - Emotion API\n",
    "You can use Emotion API to Detect, emotion recognition in image, in video.  \n",
    "Ref: https://www.microsoft.com/cognitive-services/en-us/emotion-api\n",
    "\n",
    "Below sample shows how to call Cognitive Services Emotion API to detect facial emotion in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# load image file to process\n",
    "blob_name = 'emotion1.jpeg'\n",
    "blob = blob_service.get_blob_to_bytes(container_name, blob_name)\n",
    "image_file_in_mem = io.BytesIO(blob)\n",
    "img_bytes = Image.open(image_file_in_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "\n",
    "url_emotion_api = 'https://api.projectoxford.ai/emotion/v1.0/recognize' # service address \n",
    "api_key = '8eae1ba129df43a289015fb45554e074'          # Azure Cognitive API Key, replace with your own key\n",
    "\n",
    "headers = {'Content-Type': 'application/octet-stream', 'Ocp-Apim-Subscription-Key':api_key}\n",
    "\n",
    "api_response = requests.post(url_emotion_api, headers=headers, data=blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Print out the call response (which is in Json format) in pretty form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "\n",
    "print(json.dumps(res_json, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Get detected face coordiates (bounding squares) from call response and draw borders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "ax = plt.gca()\n",
    "\n",
    "# Draw face rectangles\n",
    "for i in res_json:\n",
    "    pc = '#00FF00' # patch color\n",
    "    \n",
    "    fr = i['faceRectangle'] # get faceRectangle node per detected face in the image\n",
    "    ax.add_patch(\n",
    "        patches.Rectangle(\n",
    "            (fr['left'], fr['top']), fr['width'], fr['height'],\n",
    "            fill=False, linewidth=4, color=pc)\n",
    "    )\n",
    "    \n",
    "    s = i['scores']\n",
    "    \n",
    "    frm_txt = '{0: <10}: {1:>.6f}\\n{2: <10}: {3:>.6f}\\n{4: <10}: {5:>.6f}\\n{6: <10}: {7:>.6f}\\n\\\n",
    "{8: <10}: {9:>.6f}\\n{10: <10}: {11:>.6f}\\n{12: <10}: {13:>.6f}\\n{14: <10}: {15:>.6f}'\n",
    "    \n",
    "    lbls =     ['anger', s['anger'],\\\n",
    "                'contempt', s['contempt'],\\\n",
    "                'disgust', s['disgust'],\\\n",
    "                'fear', s['fear'],\\\n",
    "                'happiness', s['happiness'],\\\n",
    "                'neutral', s['neutral'],\\\n",
    "                'sadness', s['sadness'],\\\n",
    "                'surprise', s['surprise']]\n",
    "    \n",
    "    ax.text(fr['left'], fr['top'], frm_txt.format(*lbls), \n",
    "            fontsize=14, weight='bold', color='white', family='monospace',\n",
    "            bbox=dict(facecolor='green', alpha=0.6))    \n",
    "\n",
    "plt.imshow(img_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5. VISION - Computer Vision API - Analyze\n",
    "You can use Computer Vision API to analyze visual content in different ways based on inputs and user choices.  \n",
    "Ref: https://www.microsoft.com/cognitive-services/en-us/Computer-Vision-API/documentation \n",
    "\n",
    "Below sample shows how to call Cognitive Services Computer Vision API to detect image features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image as ipImage, display\n",
    "img_url = 'https://portalstoragewuprod.azureedge.net/vision/Analysis/7-1.jpg'\n",
    "#img_url = 'http://i.ebayimg.com/00/s/NTUxWDg3MQ==/z/IawAAOSw7NNT6NZY/$_32.JPG'\n",
    "    \n",
    "img = ipImage(url=img_url, width=250, height=250)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "\n",
    "url_vision_api = 'https://api.projectoxford.ai/vision/v1.0/analyze' # service address \n",
    "api_key = '3ae10be9a45442f5910434e1af6c52dd'          # Azure Cognitive API Key, replace with your own key\n",
    "\n",
    "headers = {'Content-Type': 'application/json', 'Ocp-Apim-Subscription-Key':api_key}\n",
    "\n",
    "params = urllib.parse.urlencode({\n",
    "    'visualFeatures': 'Categories,Tags,Description,Faces,ImageType,Color,Adult',\n",
    "})\n",
    "\n",
    "query_string = '?{0}'.format(params)\n",
    "\n",
    "url = url_vision_api + query_string\n",
    "\n",
    "body = '{\\'url\\':\\'' + img_url + '\\'}'\n",
    "\n",
    "api_response = requests.post(url, headers=headers, data=body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Print out the call response (which is in Json format) in pretty form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "\n",
    "print(json.dumps(res_json, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 6. VISION - Computer Vision API - OCR\n",
    "You can use Computer Vision API to read text in images.  \n",
    "Ref: https://www.microsoft.com/cognitive-services/en-us/Computer-Vision-API/documentation\n",
    "Below sample shows how to call Cognitive Services Computer Vision API to detect text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# load image file to process\n",
    "blob_name = 'ocr1.jpeg'\n",
    "blob = blob_service.get_blob_to_bytes(container_name, blob_name)\n",
    "image_file_in_mem = io.BytesIO(blob)\n",
    "img_bytes = Image.open(image_file_in_mem)\n",
    "\n",
    "# Show the original image that we will process\n",
    "img_url = 'https://mkmsstrg4ml.blob.core.windows.net/sampledata/ocr1.jpeg'\n",
    "img = ipImage(url=img_url, width=250, height=250)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "\n",
    "url_vision_api = 'https://api.projectoxford.ai/vision/v1.0/ocr' # service address \n",
    "api_key = '3ae10be9a45442f5910434e1af6c52dd'          # Azure Cognitive API Key, replace with your own key\n",
    "\n",
    "headers = {'Content-Type': 'application/octet-stream', 'Ocp-Apim-Subscription-Key':api_key}\n",
    "\n",
    "params = urllib.parse.urlencode({\n",
    "    'language': 'unk',\n",
    "    'detectOrientation': 'true',\n",
    "})\n",
    "\n",
    "query_string = '?{0}'.format(params)\n",
    "\n",
    "url = url_vision_api + query_string\n",
    "\n",
    "api_response = requests.post(url, headers=headers, data=blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Print out the call response (which is in Json format) in pretty form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "\n",
    "print(json.dumps(res_json, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "ax = plt.gca()\n",
    "\n",
    "text_angle = 0\n",
    "try:\n",
    "    text_angle = res_json['textAngle']\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Draw OCR rectangles\n",
    "for i in res_json['regions']:\n",
    "    pl = i['lines']\n",
    "    for k in pl:\n",
    "        words = k['words']\n",
    "        for l in words:\n",
    "            bb = l['boundingBox']\n",
    "            txt = l['text']\n",
    "            \n",
    "            bb = list(map(int, bb.split(',')))\n",
    "            \n",
    "            ax.add_patch(\n",
    "                patches.Rectangle(\n",
    "                    (bb[0], bb[1]), bb[2], bb[3], angle=text_angle,\n",
    "                    fill=False, linewidth=4, color='#00FF00')\n",
    "            )\n",
    "            \n",
    "            ax.text(bb[0], bb[1], txt, \n",
    "                    fontsize=14, weight='bold', color='red', bbox=dict(facecolor='white', alpha=0.8))\n",
    "            \n",
    "plt.imshow(img_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 7. SPEECH - BING Speech API - TTS\n",
    "\n",
    "Ref: https://www.microsoft.com/cognitive-services/en-us/speech-api/documentation/overview \n",
    "\n",
    "The Speech Recognition API provides the ability to convert text to spoken audio by sending text to Microsoftâs servers in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Get access token to use the speech services\n",
    "url_token_api = 'https://api.cognitive.microsoft.com/sts/v1.0/issueToken' # service address \n",
    "api_key = '39f61de6d28e4667bbfda88dd37b12ce'          # Azure Cognitive API Key, replace with your own key\n",
    "\n",
    "headers = {'Content-Length': '0', 'Ocp-Apim-Subscription-Key':api_key}\n",
    "\n",
    "api_response = requests.post(url_token_api, headers=headers)\n",
    "\n",
    "access_token = str(api_response.content.decode('utf-8'))\n",
    "\n",
    "\n",
    "text = '<speak version=\\'1.0\\' xml:lang=\\'en-US\\'>\\\n",
    "            <voice xml:lang=\\'en-US\\' xml:gender=\\'Female\\' \\\n",
    "                        name=\\'Microsoft Server Speech Text to Speech Voice (en-US, ZiraRUS)\\'>\\\n",
    "                The Text-To-Speech API enables you to build smart apps that can speak. You can test it now, \\\n",
    "                simply choose your target language, add your sentences then click on the play button to \\\n",
    "                see how speech synthesis works. \\\n",
    "            </voice>\\\n",
    "        </speak>'\n",
    "\n",
    "\n",
    "# Call Speech to text service\n",
    "url_tts_api = 'https://speech.platform.bing.com/synthesize' # service address \n",
    "\n",
    "headers = {'Authorization': 'Bearer {0}'.format(access_token), \\\n",
    "           'Content-Length': len(text), \\\n",
    "           'Content-type': 'text/plain; charset=utf-8',\\\n",
    "           'X-Microsoft-OutputFormat': 'riff-16khz-16bit-mono-pcm'}\n",
    "\n",
    "api_response = requests.post(url_tts_api, headers=headers, data=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Play the audio content returned by the service call above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "audio_bytes = Audio(data=api_response.content)\n",
    "display(audio_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 8. SPEECH - BING Speech API\n",
    "\n",
    "Ref: https://www.microsoft.com/cognitive-services/en-us/speech-api/documentation/overview \n",
    "\n",
    "The Speech Recognition API provides the ability to convert spoken audio to text by sending audio to Microsoftâs servers in the cloud. Developers have a choice of using the REST API or the Client Library (for real-time streaming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'container_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f61957a4b20a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# load speech file to process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mblob_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'speech3.wav'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblob_service\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_blob_to_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mwav_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'container_name' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "# load speech file to process\n",
    "blob_name = 'speech3.wav'\n",
    "blob = blob_service.get_blob_to_bytes(container_name, blob_name)\n",
    "\n",
    "wav_bytes = Audio(data=blob)\n",
    "display(wav_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'blob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3e97a4e22d3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0murl_stt_api\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://speech.platform.bing.com/recognize'\u001b[0m \u001b[0;31m# service address\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Authorization'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Bearer {0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccess_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0;34m'Content-Length'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0;34m'Content-type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'audio/wav'\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0;34m'codec'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'audio/pcm'\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0;34m'samplerate'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'16000'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m params = urllib.parse.urlencode({\n",
      "\u001b[0;31mNameError\u001b[0m: name 'blob' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import uuid\n",
    "\n",
    "# Get access token to use the speech services\n",
    "url_token_api = 'https://api.cognitive.microsoft.com/sts/v1.0/issueToken' # service address \n",
    "api_key = '39f61de6d28e4667bbfda88dd37b12ce'          # Azure Cognitive API Key, replace with your own key\n",
    "\n",
    "headers = {'Content-Length': '0', 'Ocp-Apim-Subscription-Key':api_key}\n",
    "\n",
    "api_response = requests.post(url_token_api, headers=headers)\n",
    "\n",
    "access_token = str(api_response.content.decode('utf-8'))\n",
    "\n",
    "\n",
    "# Call Speech to text service\n",
    "url_stt_api = 'https://speech.platform.bing.com/recognize' # service address \n",
    "\n",
    "headers = {'Authorization': 'Bearer {0}'.format(access_token), \\\n",
    "           'Content-Length': len(blob), \\\n",
    "           'Content-type': 'audio/wav', \\\n",
    "           'codec': 'audio/pcm', \\\n",
    "           'samplerate': '16000'}\n",
    "\n",
    "params = urllib.parse.urlencode({\n",
    "    'scenarios': 'ulm',\n",
    "    'appid': 'D4D52672-91D7-4C74-8AD8-42B1D98141A5', # dont change, it is fixed by design\n",
    "    'locale': 'en-US', # speech in english\n",
    "    'device.os': 'PC',\n",
    "    'version': '3.0',\n",
    "    'format': 'json', # return value in json\n",
    "    'instanceid': str(uuid.uuid1()), # any guid\n",
    "    'requestid': str(uuid.uuid1()),\n",
    "})\n",
    "\n",
    "api_response = requests.post(url_stt_api, headers=headers, params=params, data=blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "\n",
    "print(json.dumps(res_json, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 9. LANGUAGE - BING Text Analytics API - Sentiment, Key Phrases, Languages\n",
    "Extracts information from your text.\n",
    "ref: https://azure.microsoft.com/en-us/documentation/articles/cognitive-services-text-analytics-quick-start/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Get access token to use the speech services\n",
    "url_sentiment_api = 'https://westus.api.cognitive.microsoft.com/text/analytics/v2.0/sentiment' # service address \n",
    "url_keyphrases_api = 'https://westus.api.cognitive.microsoft.com/text/analytics/v2.0/keyPhrases' # service address \n",
    "url_languages_api = 'https://westus.api.cognitive.microsoft.com/text/analytics/v2.0/languages' # service address \n",
    "api_key = 'c97786c34cc24041aedfa2a1c64d9aee'          # Azure Cognitive API Key, replace with your own key\n",
    "\n",
    "headers = {'Content-Length': '0', 'Ocp-Apim-Subscription-Key':api_key}\n",
    "\n",
    "text = '{                                           \\\n",
    "            \"documents\": [                          \\\n",
    "                {                                   \\\n",
    "                    \"language\": \"en\",               \\\n",
    "                    \"id\": \"1\",                      \\\n",
    "                    \"text\": \"remember where we were eight years ago we had the worst financial crisis \\\n",
    "                    the great recession the worst since the nineteen thirties\"        \\\n",
    "                },                                  \\\n",
    "            ]                                       \\\n",
    "        }'                                          \\\n",
    "\n",
    "\n",
    "headers = {'Ocp-Apim-Subscription-Key':api_key, \\\n",
    "           'Content-type': 'application/json',\\\n",
    "           'Accept': 'application/json'}\n",
    "\n",
    "api_response_sentiment = requests.post(url_sentiment_api, headers=headers, data=text)\n",
    "api_response_keyphrases = requests.post(url_keyphrases_api, headers=headers, data=text)\n",
    "api_response_languages = requests.post(url_languages_api, headers=headers, data=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "res_json_sentiment = json.loads(api_response_sentiment.content.decode('utf-8'))\n",
    "res_json_keyphrases = json.loads(api_response_keyphrases.content.decode('utf-8'))\n",
    "res_json_languages = json.loads(api_response_languages.content.decode('utf-8'))\n",
    "\n",
    "print('// Sentiment analysis result:')\n",
    "print(json.dumps(res_json_sentiment, indent=2, sort_keys=True))\n",
    "print('\\n\\n// Key phrase analysis result:')\n",
    "print(json.dumps(res_json_keyphrases, indent=2, sort_keys=True))\n",
    "print('\\n\\n//Language analysis result:')\n",
    "print(json.dumps(res_json_languages, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 10: Microsoft Academic Knowledge API\n",
    "\n",
    "As a first step, sign up for access to the API and key your access keys:\n",
    "    \n",
    "https://azure.microsoft.com/en-gb/try/cognitive-services/?api=academic-knowledge-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
