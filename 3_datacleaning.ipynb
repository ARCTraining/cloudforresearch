{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = 'trump_new.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"created_at\": \"Fri Mar 24 13:00:23 +0000 2017\",\n",
      "    \"place\": null,\n",
      "    \"source\": \"<a href=\\\"https://path.com/\\\" rel=\\\"nofollow\\\">Path</a>\",\n",
      "    \"favorited\": false,\n",
      "    \"retweeted_status\": {\n",
      "        \"created_at\": \"Fri Mar 24 12:59:58 +0000 2017\",\n",
      "        \"place\": null,\n",
      "        \"source\": \"<a href=\\\"http://www.thebeltwaypundit.com\\\" rel=\\\"nofollow\\\">PostFromMyMac</a>\",\n",
      "        \"is_quote_status\": false,\n",
      "        \"favorited\": false,\n",
      "        \"text\": \"#TRUMP #REGRETS: SHOULD HAVE DONE #TAX CUTS FIRST... https://t.co/9B1gbsC2bW\",\n",
      "        \"truncated\": false,\n",
      "        \"retweet_count\": 25,\n",
      "        \"in_reply_to_status_id_str\": null,\n",
      "        \"id_str\": \"845258889549504512\",\n",
      "        \"coordinates\": null,\n",
      "        \"lang\": \"en\",\n",
      "        \"id\": 845258889549504512,\n",
      "        \"in_reply_to_status_id\": null,\n",
      "        \"possibly_sensitive\": false,\n",
      "        \"in_reply_to_user_id_str\": null,\n",
      "        \"in_reply_to_user_id\": null,\n",
      "        \"contributors\": null,\n",
      "        \"in_reply_to_screen_name\": null,\n",
      "        \"entities\": {\n",
      "            \"urls\": [\n",
      "                {\n",
      "                    \"expanded_url\": \"https://www.nytimes.com/2017/03/23/us/politics/trump-health-care-bill-regrets.html\",\n",
      "                    \"indices\": [\n",
      "                        53,\n",
      "                        76\n",
      "                    ],\n",
      "                    \"display_url\": \"nytimes.com/2017/03/23/us/\\u2026\",\n",
      "                    \"url\": \"https://t.co/9B1gbsC2bW\"\n",
      "                }\n",
      "            ],\n",
      "            \"user_mentions\": [],\n",
      "            \"hashtags\": [\n",
      "                {\n",
      "                    \"indices\": [\n",
      "                        0,\n",
      "                        6\n",
      "                    ],\n",
      "                    \"text\": \"TRUMP\"\n",
      "                },\n",
      "                {\n",
      "                    \"indices\": [\n",
      "                        7,\n",
      "                        15\n",
      "                    ],\n",
      "                    \"text\": \"REGRETS\"\n",
      "                },\n",
      "                {\n",
      "                    \"indices\": [\n",
      "                        34,\n",
      "                        38\n",
      "                    ],\n",
      "                    \"text\": \"TAX\"\n",
      "                }\n",
      "            ],\n",
      "            \"symbols\": []\n",
      "        },\n",
      "        \"user\": {\n",
      "            \"created_at\": \"Thu May 17 19:48:27 +0000 2012\",\n",
      "            \"favourites_count\": 75,\n",
      "            \"default_profile_image\": false,\n",
      "            \"default_profile\": true,\n",
      "            \"url\": null,\n",
      "            \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/798945980221685765/MXEWFGsT_normal.jpg\",\n",
      "            \"name\": \"Beltway Pundit\",\n",
      "            \"time_zone\": \"Eastern Time (US & Canada)\",\n",
      "            \"is_translator\": false,\n",
      "            \"profile_sidebar_border_color\": \"C0DEED\",\n",
      "            \"profile_link_color\": \"1DA1F2\",\n",
      "            \"profile_background_image_url_https\": \"https://abs.twimg.com/images/themes/theme1/bg.png\",\n",
      "            \"friends_count\": 21,\n",
      "            \"follow_request_sent\": null,\n",
      "            \"profile_background_tile\": false,\n",
      "            \"protected\": false,\n",
      "            \"listed_count\": 202,\n",
      "            \"id\": 583162248,\n",
      "            \"notifications\": null,\n",
      "            \"contributors_enabled\": false,\n",
      "            \"verified\": false,\n",
      "            \"lang\": \"en\",\n",
      "            \"following\": null,\n",
      "            \"utc_offset\": -14400,\n",
      "            \"location\": \"Florida, USA\",\n",
      "            \"description\": null,\n",
      "            \"profile_sidebar_fill_color\": \"DDEEF6\",\n",
      "            \"followers_count\": 97487,\n",
      "            \"profile_image_url\": \"http://pbs.twimg.com/profile_images/798945980221685765/MXEWFGsT_normal.jpg\",\n",
      "            \"profile_background_color\": \"C0DEED\",\n",
      "            \"id_str\": \"583162248\",\n",
      "            \"statuses_count\": 11975,\n",
      "            \"profile_text_color\": \"333333\",\n",
      "            \"screen_name\": \"BeltwayPundit\",\n",
      "            \"profile_background_image_url\": \"http://abs.twimg.com/images/themes/theme1/bg.png\",\n",
      "            \"geo_enabled\": false,\n",
      "            \"profile_use_background_image\": true\n",
      "        },\n",
      "        \"retweeted\": false,\n",
      "        \"geo\": null,\n",
      "        \"filter_level\": \"low\",\n",
      "        \"favorite_count\": 24\n",
      "    },\n",
      "    \"is_quote_status\": false,\n",
      "    \"favorite_count\": 0,\n",
      "    \"text\": \"RT @BeltwayPundit: #TRUMP #REGRETS: SHOULD HAVE DONE #TAX CUTS FIRST... https://t.co/9B1gbsC2bW\",\n",
      "    \"truncated\": false,\n",
      "    \"retweet_count\": 0,\n",
      "    \"in_reply_to_status_id_str\": null,\n",
      "    \"id_str\": \"845258996126670849\",\n",
      "    \"coordinates\": null,\n",
      "    \"lang\": \"en\",\n",
      "    \"id\": 845258996126670849,\n",
      "    \"in_reply_to_status_id\": null,\n",
      "    \"possibly_sensitive\": false,\n",
      "    \"in_reply_to_user_id_str\": null,\n",
      "    \"timestamp_ms\": \"1490360423486\",\n",
      "    \"in_reply_to_user_id\": null,\n",
      "    \"contributors\": null,\n",
      "    \"in_reply_to_screen_name\": null,\n",
      "    \"entities\": {\n",
      "        \"urls\": [\n",
      "            {\n",
      "                \"expanded_url\": \"https://www.nytimes.com/2017/03/23/us/politics/trump-health-care-bill-regrets.html\",\n",
      "                \"indices\": [\n",
      "                    72,\n",
      "                    95\n",
      "                ],\n",
      "                \"display_url\": \"nytimes.com/2017/03/23/us/\\u2026\",\n",
      "                \"url\": \"https://t.co/9B1gbsC2bW\"\n",
      "            }\n",
      "        ],\n",
      "        \"user_mentions\": [\n",
      "            {\n",
      "                \"id\": 583162248,\n",
      "                \"id_str\": \"583162248\",\n",
      "                \"indices\": [\n",
      "                    3,\n",
      "                    17\n",
      "                ],\n",
      "                \"name\": \"Beltway Pundit\",\n",
      "                \"screen_name\": \"BeltwayPundit\"\n",
      "            }\n",
      "        ],\n",
      "        \"hashtags\": [\n",
      "            {\n",
      "                \"indices\": [\n",
      "                    19,\n",
      "                    25\n",
      "                ],\n",
      "                \"text\": \"TRUMP\"\n",
      "            },\n",
      "            {\n",
      "                \"indices\": [\n",
      "                    26,\n",
      "                    34\n",
      "                ],\n",
      "                \"text\": \"REGRETS\"\n",
      "            },\n",
      "            {\n",
      "                \"indices\": [\n",
      "                    53,\n",
      "                    57\n",
      "                ],\n",
      "                \"text\": \"TAX\"\n",
      "            }\n",
      "        ],\n",
      "        \"symbols\": []\n",
      "    },\n",
      "    \"user\": {\n",
      "        \"created_at\": \"Fri Mar 13 16:57:56 +0000 2015\",\n",
      "        \"favourites_count\": 1149,\n",
      "        \"default_profile_image\": false,\n",
      "        \"default_profile\": true,\n",
      "        \"url\": null,\n",
      "        \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/579531215159586816/BFWmNKKT_normal.jpg\",\n",
      "        \"name\": \"Maryline Bunul\",\n",
      "        \"time_zone\": null,\n",
      "        \"is_translator\": false,\n",
      "        \"profile_sidebar_border_color\": \"C0DEED\",\n",
      "        \"profile_link_color\": \"1DA1F2\",\n",
      "        \"profile_background_image_url_https\": \"https://abs.twimg.com/images/themes/theme1/bg.png\",\n",
      "        \"friends_count\": 4,\n",
      "        \"follow_request_sent\": null,\n",
      "        \"profile_background_tile\": false,\n",
      "        \"protected\": false,\n",
      "        \"listed_count\": 0,\n",
      "        \"id\": 3077446141,\n",
      "        \"notifications\": null,\n",
      "        \"contributors_enabled\": false,\n",
      "        \"verified\": false,\n",
      "        \"lang\": \"en\",\n",
      "        \"following\": null,\n",
      "        \"utc_offset\": null,\n",
      "        \"location\": \"Diest\\r\\n\",\n",
      "        \"description\": \"Typical zombie junkie. Unable to type with boxing gloves on. Unapologetic coffee evangelist.\",\n",
      "        \"profile_sidebar_fill_color\": \"DDEEF6\",\n",
      "        \"followers_count\": 138,\n",
      "        \"profile_image_url\": \"http://pbs.twimg.com/profile_images/579531215159586816/BFWmNKKT_normal.jpg\",\n",
      "        \"profile_background_color\": \"C0DEED\",\n",
      "        \"id_str\": \"3077446141\",\n",
      "        \"statuses_count\": 1040,\n",
      "        \"profile_text_color\": \"333333\",\n",
      "        \"screen_name\": \"94_tyamanova\",\n",
      "        \"profile_background_image_url\": \"http://abs.twimg.com/images/themes/theme1/bg.png\",\n",
      "        \"geo_enabled\": false,\n",
      "        \"profile_use_background_image\": true\n",
      "    },\n",
      "    \"retweeted\": false,\n",
      "    \"geo\": null,\n",
      "    \"filter_level\": \"low\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    " \n",
    "with open(fname, 'r') as f:\n",
    "    line = f.readline() # read only the first tweet/line\n",
    "    tweet = json.loads(line) # load it as Python dict\n",
    "    print(json.dumps(tweet, indent=4)) # pretty-print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): nltk in /home/nbcommon/anaconda3_410/lib/python3.5/site-packages\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/home/nbuser/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4af891a4b792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'RT @callaghanmt: just an example! :D http://example.com #NLP'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/nbuser/anaconda3_410/lib/python3.5/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     return [token for sent in sent_tokenize(text, language)\n\u001b[0m\u001b[1;32m    107\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nbuser/anaconda3_410/lib/python3.5/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \"\"\"\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nbuser/anaconda3_410/lib/python3.5/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nbuser/anaconda3_410/lib/python3.5/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nbuser/anaconda3_410/lib/python3.5/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/home/nbuser/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "#Tokenise a tweet\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "tweet = 'RT @callaghanmt: just an example! :D http://example.com #NLP'\n",
    "print(word_tokenize(tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A better tokeniser, taking into account twitter type tokens\n",
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@callaghanmt', ':', 'just', 'an', 'example', '!', ':D', 'http://example.com', '#NLP']\n"
     ]
    }
   ],
   "source": [
    "tweet = 'RT @callaghanmt: just an example! :D http://example.com #NLP'\n",
    "print(preprocess(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':', 1898), ('.', 1718), ('#Trump', 1478), ('RT', 1230), ('…', 1096)]\n"
     ]
    }
   ],
   "source": [
    "#find common terms\n",
    "import operator \n",
    "import json\n",
    "from collections import Counter\n",
    " \n",
    "with open(fname, 'r') as f:\n",
    "    count_all = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        # Create a list with all the terms\n",
    "        terms_all = [term for term in preprocess(tweet['text'])]\n",
    "        # Update the counter\n",
    "        count_all.update(terms_all)\n",
    "    # Print the first 5 most frequent words\n",
    "    print(count_all.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove some stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via', '…', 'RT', 'amp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#apply this\n",
    "terms_stop = [term for term in preprocess(tweet['text']) if term not in stop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#Brexit', 624), ('#brexit', 153), ('EU', 144), ('Brexit', 126), ('The', 87)]\n"
     ]
    }
   ],
   "source": [
    "#find common terms\n",
    "import operator \n",
    "import json\n",
    "from collections import Counter\n",
    " \n",
    "with open(fname, 'r') as f:\n",
    "    count_all = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        # Create a list with all the terms\n",
    "        terms_stop = [term for term in preprocess(tweet['text']) if term not in stop]\n",
    "        # Update the counter\n",
    "        count_all.update(terms_stop)\n",
    "    # Print the first 5 most frequent words\n",
    "    print(count_all.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now look at bigrams\n",
    "from nltk import bigrams \n",
    " \n",
    "terms_bigram = bigrams(terms_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('#BREXIT', '#StrongerIn'), 49), (('#StrongerIn', '#No2EU'), 43), (('#No2EU', '#EUref'), 41), (('#EUref', '#LeaveEU'), 35), (('#Trump', '#LePen'), 29), (('leave', 'EU'), 28), (('Must-read', 'book'), 27), (('#iceberg', 'Must-read'), 27), (('Populist', '#iceberg'), 27), (('Just', 'times'), 27), (('#LePen', '#factcheck'), 27), (('book', '@jnpaquet'), 27), (('In', 'campaign'), 27), (('#Brexit', 'Tip'), 27), (('confirmed', 'If'), 27), (('Tip', 'Populist'), 27), (('PLEASE', 'Just'), 27), (('campaign', 'confirmed'), 27), (('If', 'leave'), 27), (('times', 'Stronger'), 27)]\n"
     ]
    }
   ],
   "source": [
    "import operator \n",
    "import json\n",
    "from collections import Counter\n",
    "from nltk import bigrams \n",
    "  \n",
    "with open(fname, 'r') as f:\n",
    "    count_all = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        # Create a list with all the terms\n",
    "        terms_stop = [term for term in preprocess(tweet['text']) if term not in stop]\n",
    "        terms_bigram = bigrams(terms_stop)\n",
    "        # Update the counter\n",
    "        count_all.update(terms_bigram)\n",
    "    # Print the first 5 most frequent words\n",
    "    print(count_all.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at term co-occurences\n",
    "#removing # and @ tokens\n",
    "from collections import defaultdict\n",
    "# remember to include the other import from the previous post\n",
    " \n",
    "com = defaultdict(lambda : defaultdict(int))\n",
    "\n",
    "with open(fname, 'r') as f:\n",
    "# f is the file pointer to the JSON data set\n",
    "    for line in f: \n",
    "        count_terms_only = Counter()\n",
    "        tweet = json.loads(line)\n",
    "        terms_only = [term for term in preprocess(tweet['text']) \n",
    "                  if term not in stop \n",
    "                  and not term.startswith(('#', '@'))]\n",
    " \n",
    "        # Build co-occurrence matrix\n",
    "        for i in range(len(terms_only)-1):            \n",
    "            for j in range(i+1, len(terms_only)):\n",
    "                w1, w2 = sorted([terms_only[i], terms_only[j]])                \n",
    "                if w1 != w2:\n",
    "                    com[w1][w2] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('Just', 'leave'), 54), (('Stronger', 'leave'), 54), (('If', 'leave'), 54), (('leave', 'times'), 54), (('PLEASE', 'leave'), 54), (('confirmed', 'leave'), 54), (('campaign', 'leave'), 54), (('In', 'leave'), 54), (('EU', 'leave'), 46), (('Mar', 'leave'), 36), (('Single', 'leave'), 36), (('boards', 'workers'), 30), (('cliff', 'edge'), 30), (('Brexit', 'May'), 29), (('means', 'workers'), 28)]\n"
     ]
    }
   ],
   "source": [
    "com_max = []\n",
    "# For each term, look for the most common co-occurrent terms\n",
    "for t1 in com:\n",
    "    t1_max_terms = sorted(com[t1].items(), key=operator.itemgetter(1), reverse=True)[:5]\n",
    "    for t2, t2_count in t1_max_terms:\n",
    "        com_max.append(((t1, t2), t2_count))\n",
    "# Get the most frequent co-occurrences\n",
    "terms_max = sorted(com_max, key=operator.itemgetter(1), reverse=True)\n",
    "print(terms_max[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vincent\n",
    "from vincent import AxisProperties, PropertySet, ValueRef\n",
    " \n",
    "word_freq = count_all.most_common(20)\n",
    "labels, freq = zip(*word_freq)\n",
    "data = {'data': freq, 'x': labels}\n",
    "bar = vincent.Bar(data, iter_idx='x')\n",
    "ax = AxisProperties(\n",
    "         labels = PropertySet(angle=ValueRef(value=90)))\n",
    "bar.axes[0].properties = ax\n",
    "bar.to_json('term_freq.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now view in the web browser by running the python web server:\n",
    "# python -m http.server 6789\n",
    "\n",
    "# then in browser, do:\n",
    "# http://localhost:6789/chart.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of approaches to sentiment analysis- take this one for example:\n",
    "\n",
    "http://arxiv.org/abs/cs/0212032\n",
    "\n",
    "*Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews.*\n",
    "\n",
    "Peter Turney\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
